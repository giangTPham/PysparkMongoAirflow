Spark Executor Command: "/home/gitpod/.sdkman/candidates/java/current/bin/java" "-cp" "/workspace/PysparkMongoAirflow/spark-3.1.1-bin-hadoop2.7/conf/:/workspace/PysparkMongoAirflow/spark-3.1.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=36171" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@10.0.2.100:36171" "--executor-id" "0" "--hostname" "10.0.2.100" "--cores" "1" "--app-id" "app-20220302074216-0003" "--worker-url" "spark://Worker@10.0.2.100:36917"
========================================

Picked up JAVA_TOOL_OPTIONS:  -Xmx3435m
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/03/02 07:42:17 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 26674@giangtpham-pysparkmongoa-8e4ves7r5or
22/03/02 07:42:17 INFO SignalUtils: Registering signal handler for TERM
22/03/02 07:42:17 INFO SignalUtils: Registering signal handler for HUP
22/03/02 07:42:17 INFO SignalUtils: Registering signal handler for INT
22/03/02 07:42:17 WARN Utils: Your hostname, giangtpham-pysparkmongoa-8e4ves7r5or resolves to a loopback address: 127.0.0.1; using 10.0.2.100 instead (on interface tap0)
22/03/02 07:42:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/workspace/PysparkMongoAirflow/spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/03/02 07:42:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/03/02 07:42:17 INFO SecurityManager: Changing view acls to: gitpod
22/03/02 07:42:17 INFO SecurityManager: Changing modify acls to: gitpod
22/03/02 07:42:17 INFO SecurityManager: Changing view acls groups to: 
22/03/02 07:42:17 INFO SecurityManager: Changing modify acls groups to: 
22/03/02 07:42:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gitpod); groups with view permissions: Set(); users  with modify permissions: Set(gitpod); groups with modify permissions: Set()
22/03/02 07:42:18 INFO TransportClientFactory: Successfully created connection to /10.0.2.100:36171 after 88 ms (0 ms spent in bootstraps)
22/03/02 07:42:18 INFO SecurityManager: Changing view acls to: gitpod
22/03/02 07:42:18 INFO SecurityManager: Changing modify acls to: gitpod
22/03/02 07:42:18 INFO SecurityManager: Changing view acls groups to: 
22/03/02 07:42:18 INFO SecurityManager: Changing modify acls groups to: 
22/03/02 07:42:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gitpod); groups with view permissions: Set(); users  with modify permissions: Set(gitpod); groups with modify permissions: Set()
22/03/02 07:42:18 INFO TransportClientFactory: Successfully created connection to /10.0.2.100:36171 after 2 ms (0 ms spent in bootstraps)
22/03/02 07:42:18 INFO DiskBlockManager: Created local directory at /tmp/spark-4b8a5c4f-fd59-4e7b-b712-4d7efa021ada/executor-c8d45d34-a2cf-4674-b789-237490c749ac/blockmgr-01e98077-98cd-4672-8ec4-4c9458d758ea
22/03/02 07:42:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
22/03/02 07:42:18 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.2.100:36171
22/03/02 07:42:18 INFO WorkerWatcher: Connecting to worker spark://Worker@10.0.2.100:36917
22/03/02 07:42:18 INFO TransportClientFactory: Successfully created connection to /10.0.2.100:36917 after 2 ms (0 ms spent in bootstraps)
22/03/02 07:42:18 INFO WorkerWatcher: Successfully connected to spark://Worker@10.0.2.100:36917
22/03/02 07:42:18 INFO ResourceUtils: ==============================================================
22/03/02 07:42:18 INFO ResourceUtils: No custom resources configured for spark.executor.
22/03/02 07:42:18 INFO ResourceUtils: ==============================================================
22/03/02 07:42:18 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
22/03/02 07:42:18 INFO Executor: Starting executor ID 0 on host 10.0.2.100
22/03/02 07:42:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45121.
22/03/02 07:42:19 INFO NettyBlockTransferService: Server created on 10.0.2.100:45121
22/03/02 07:42:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/03/02 07:42:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 10.0.2.100, 45121, None)
22/03/02 07:42:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 10.0.2.100, 45121, None)
22/03/02 07:42:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 10.0.2.100, 45121, None)
22/03/02 07:42:20 INFO CoarseGrainedExecutorBackend: Got assigned task 0
22/03/02 07:42:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/03/02 07:42:21 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:21 INFO TransportClientFactory: Successfully created connection to /10.0.2.100:46251 after 2 ms (0 ms spent in bootstraps)
22/03/02 07:42:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 434.4 MiB)
22/03/02 07:42:21 INFO TorrentBroadcast: Reading broadcast variable 1 took 118 ms
22/03/02 07:42:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 434.4 MiB)
22/03/02 07:42:21 INFO CodeGenerator: Code generated in 197.705513 ms
22/03/02 07:42:21 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/promotions/2021-11-01/part-00000-802e6ae2-b27d-463b-a5c0-5b2d91ad831f-c000.csv, range: 0-51809, partition values: [empty row]
22/03/02 07:42:21 INFO CodeGenerator: Code generated in 10.28846 ms
22/03/02 07:42:21 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.4 MiB)
22/03/02 07:42:21 INFO TorrentBroadcast: Reading broadcast variable 0 took 12 ms
22/03/02 07:42:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.5 KiB, free 434.2 MiB)
22/03/02 07:42:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1607 bytes result sent to driver
22/03/02 07:42:22 INFO CoarseGrainedExecutorBackend: Got assigned task 1
22/03/02 07:42:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/03/02 07:42:22 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.2 MiB)
22/03/02 07:42:22 INFO TorrentBroadcast: Reading broadcast variable 3 took 11 ms
22/03/02 07:42:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.1 MiB)
22/03/02 07:42:23 INFO CodeGenerator: Code generated in 10.070995 ms
22/03/02 07:42:23 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/promotions/2021-11-01/part-00000-802e6ae2-b27d-463b-a5c0-5b2d91ad831f-c000.csv, range: 0-51809, partition values: [empty row]
22/03/02 07:42:23 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.1 MiB)
22/03/02 07:42:23 INFO TorrentBroadcast: Reading broadcast variable 2 took 11 ms
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.5 KiB, free 433.9 MiB)
22/03/02 07:42:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1657 bytes result sent to driver
22/03/02 07:42:23 INFO CoarseGrainedExecutorBackend: Got assigned task 2
22/03/02 07:42:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/03/02 07:42:23 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 433.9 MiB)
22/03/02 07:42:23 INFO TorrentBroadcast: Reading broadcast variable 5 took 10 ms
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.8 KiB, free 433.9 MiB)
22/03/02 07:42:23 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/transactions/2021-11-01/part-00000-8f0661ad-96d1-41c9-9b42-0d887254a35f-c000.csv, range: 0-251273, partition values: [empty row]
22/03/02 07:42:23 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 433.9 MiB)
22/03/02 07:42:23 INFO TorrentBroadcast: Reading broadcast variable 4 took 13 ms
22/03/02 07:42:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.5 KiB, free 433.7 MiB)
22/03/02 07:42:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1577 bytes result sent to driver
22/03/02 07:42:24 INFO CoarseGrainedExecutorBackend: Got assigned task 3
22/03/02 07:42:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 7 took 10 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.6 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/transactions/2021-11-01/part-00000-8f0661ad-96d1-41c9-9b42-0d887254a35f-c000.csv, range: 0-251273, partition values: [empty row]
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 433.6 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 6 took 10 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.5 KiB, free 433.4 MiB)
22/03/02 07:42:24 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1629 bytes result sent to driver
22/03/02 07:42:24 INFO CoarseGrainedExecutorBackend: Got assigned task 4
22/03/02 07:42:24 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 9 took 7 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 10.8 KiB, free 433.6 MiB)
22/03/02 07:42:24 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/users/2021-11-01/part-00000-90950e0e-e0ae-4b04-a8ba-e2e6d8420f9e-c000.csv, range: 0-19126, partition values: [empty row]
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 433.9 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 8 took 9 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.5 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1563 bytes result sent to driver
22/03/02 07:42:24 INFO CoarseGrainedExecutorBackend: Got assigned task 5
22/03/02 07:42:24 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.2 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 11 took 7 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 15.6 KiB, free 434.1 MiB)
22/03/02 07:42:24 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/users/2021-11-01/part-00000-90950e0e-e0ae-4b04-a8ba-e2e6d8420f9e-c000.csv, range: 0-19126, partition values: [empty row]
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 434.1 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 10 took 8 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.5 KiB, free 433.9 MiB)
22/03/02 07:42:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1571 bytes result sent to driver
22/03/02 07:42:24 INFO CoarseGrainedExecutorBackend: Got assigned task 6
22/03/02 07:42:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 56.1 KiB, free 433.9 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 13 took 7 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 156.1 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/promotions/2021-11-01/part-00000-802e6ae2-b27d-463b-a5c0-5b2d91ad831f-c000.csv, range: 0-51809, partition values: [empty row]
22/03/02 07:42:24 INFO CodeGenerator: Code generated in 11.527993 ms
22/03/02 07:42:24 INFO TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 433.7 MiB)
22/03/02 07:42:24 INFO TorrentBroadcast: Reading broadcast variable 12 took 9 ms
22/03/02 07:42:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.5 KiB, free 433.5 MiB)
22/03/02 07:42:24 INFO MemoryStore: Block rdd_33_0 stored as values in memory (estimated size 46.2 KiB, free 433.4 MiB)
22/03/02 07:42:24 INFO CodeGenerator: Code generated in 5.062355 ms
22/03/02 07:42:24 INFO CodeGenerator: Code generated in 26.080305 ms
22/03/02 07:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:24 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:24 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet block size to 134217728
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Dictionary is on
22/03/02 07:42:25 INFO ParquetOutputFormat: Validation is off
22/03/02 07:42:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/03/02 07:42:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/03/02 07:42:25 INFO ParquetOutputFormat: Page size checking is: estimated
22/03/02 07:42:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/03/02 07:42:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/03/02 07:42:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "userid",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "voucherCode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignID",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 userid;
  optional binary voucherCode (UTF8);
  optional binary status (UTF8);
  optional int32 campaignID;
  optional binary time (UTF8);
}

       
22/03/02 07:42:25 INFO CodecPool: Got brand-new compressor [.snappy]
22/03/02 07:42:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 65464
22/03/02 07:42:25 INFO FileOutputCommitter: Saved output of task 'attempt_202203020742246472029353042858143_0006_m_000000_6' to file:/workspace/PysparkMongoAirflow/Final_Project/data/datalake/2021-11-01/promotion/_temporary/0/task_202203020742246472029353042858143_0006_m_000000
22/03/02 07:42:25 INFO SparkHadoopMapRedUtil: attempt_202203020742246472029353042858143_0006_m_000000_6: Committed
22/03/02 07:42:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2601 bytes result sent to driver
22/03/02 07:42:25 INFO CoarseGrainedExecutorBackend: Got assigned task 7
22/03/02 07:42:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
22/03/02 07:42:25 INFO TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 56.5 KiB, free 433.4 MiB)
22/03/02 07:42:25 INFO TorrentBroadcast: Reading broadcast variable 15 took 7 ms
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 156.9 KiB, free 433.2 MiB)
22/03/02 07:42:25 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/transactions/2021-11-01/part-00000-8f0661ad-96d1-41c9-9b42-0d887254a35f-c000.csv, range: 0-251273, partition values: [empty row]
22/03/02 07:42:25 INFO CodeGenerator: Code generated in 13.647562 ms
22/03/02 07:42:25 INFO TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 433.2 MiB)
22/03/02 07:42:25 INFO TorrentBroadcast: Reading broadcast variable 14 took 8 ms
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.5 KiB, free 433.0 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 233.0 KiB, free 432.8 MiB)
22/03/02 07:42:25 INFO CodeGenerator: Code generated in 20.715012 ms
22/03/02 07:42:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:25 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:25 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet block size to 134217728
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Dictionary is on
22/03/02 07:42:25 INFO ParquetOutputFormat: Validation is off
22/03/02 07:42:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/03/02 07:42:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/03/02 07:42:25 INFO ParquetOutputFormat: Page size checking is: estimated
22/03/02 07:42:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/03/02 07:42:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/03/02 07:42:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "transId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "transStatus",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "userId",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "transactionTime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "appId",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "transType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pmcId",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary transId (UTF8);
  optional int32 transStatus;
  optional int32 userId;
  optional binary transactionTime (UTF8);
  optional int32 appId;
  optional int32 transType;
  optional int32 amount;
  optional int32 pmcId;
}

       
22/03/02 07:42:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 330222
22/03/02 07:42:25 INFO FileOutputCommitter: Saved output of task 'attempt_202203020742252960108775540114272_0007_m_000000_7' to file:/workspace/PysparkMongoAirflow/Final_Project/data/datalake/2021-11-01/transaction/_temporary/0/task_202203020742252960108775540114272_0007_m_000000
22/03/02 07:42:25 INFO SparkHadoopMapRedUtil: attempt_202203020742252960108775540114272_0007_m_000000_7: Committed
22/03/02 07:42:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2515 bytes result sent to driver
22/03/02 07:42:25 INFO CoarseGrainedExecutorBackend: Got assigned task 8
22/03/02 07:42:25 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
22/03/02 07:42:25 INFO TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 56.1 KiB, free 432.7 MiB)
22/03/02 07:42:25 INFO TorrentBroadcast: Reading broadcast variable 17 took 7 ms
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 156.1 KiB, free 432.6 MiB)
22/03/02 07:42:25 INFO FileScanRDD: Reading File path: file:///workspace/PysparkMongoAirflow/Final_Project/data/source/users/2021-11-01/part-00000-90950e0e-e0ae-4b04-a8ba-e2e6d8420f9e-c000.csv, range: 0-19126, partition values: [empty row]
22/03/02 07:42:25 INFO CodeGenerator: Code generated in 10.286338 ms
22/03/02 07:42:25 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 432.6 MiB)
22/03/02 07:42:25 INFO TorrentBroadcast: Reading broadcast variable 16 took 8 ms
22/03/02 07:42:25 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 201.5 KiB, free 432.4 MiB)
22/03/02 07:42:25 INFO MemoryStore: Block rdd_51_0 stored as values in memory (estimated size 20.5 KiB, free 432.3 MiB)
22/03/02 07:42:25 INFO CodeGenerator: Code generated in 13.557348 ms
22/03/02 07:42:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/03/02 07:42:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/03/02 07:42:25 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:25 INFO CodecConfig: Compression: SNAPPY
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet block size to 134217728
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/03/02 07:42:25 INFO ParquetOutputFormat: Dictionary is on
22/03/02 07:42:25 INFO ParquetOutputFormat: Validation is off
22/03/02 07:42:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/03/02 07:42:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/03/02 07:42:25 INFO ParquetOutputFormat: Page size checking is: estimated
22/03/02 07:42:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/03/02 07:42:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/03/02 07:42:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "userid",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "birthdate",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "profileLevel",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gender",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "updatedTime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 userid;
  optional binary birthdate (UTF8);
  optional int32 profileLevel;
  optional int32 gender;
  optional binary updatedTime (UTF8);
}

       
22/03/02 07:42:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 27660
22/03/02 07:42:26 INFO FileOutputCommitter: Saved output of task 'attempt_202203020742253192290114094186988_0008_m_000000_8' to file:/workspace/PysparkMongoAirflow/Final_Project/data/datalake/2021-11-01/user/_temporary/0/task_202203020742253192290114094186988_0008_m_000000
22/03/02 07:42:26 INFO SparkHadoopMapRedUtil: attempt_202203020742253192290114094186988_0008_m_000000_8: Committed
22/03/02 07:42:26 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2515 bytes result sent to driver
22/03/02 07:42:26 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
22/03/02 07:42:26 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
